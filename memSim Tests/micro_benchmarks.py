#!/usr/bin/env python3
"""
Micro-benchmarks for specific algorithm behaviors
These tests are designed to highlight specific strengths/weaknesses
"""

import subprocess
import os
import matplotlib.pyplot as plt
import json
from typing import List, Dict

class MicroBenchmark:
    """Micro-benchmark runner for algorithm analysis"""
    
    def __init__(self, memsim_path="./memSim"):
        self.memsim_path = memsim_path
        self.results = {}
    
    def run_single_test(self, name: str, addresses: List[int], frames: int, algorithm: str):
        """Run a single test configuration"""
        test_file = f"micro_{name}_{algorithm}_{frames}.txt"
        
        with open(test_file, 'w') as f:
            for addr in addresses:
                f.write(f"{addr}\n")
        
        try:
            result = subprocess.run([self.memsim_path, test_file, str(frames), algorithm], 
                                  capture_output=True, text=True)
            
            lines = result.stdout.strip().split('\n')
            page_faults = 0
            
            for line in lines:
                if line.startswith("Page Faults ="):
                    page_faults = int(line.split('=')[1].strip())
                    break
            
            return page_faults
            
        finally:
            if os.path.exists(test_file):
                os.remove(test_file)
    
    def test_stack_distance(self):
        """Test with varying stack distances (temporal locality)"""
        print("ðŸ”¬ Micro-benchmark: Stack Distance Analysis")
        
        results = {algo: [] for algo in ["FIFO", "LRU", "OPT"]}
        distances = range(1, 16)  # Stack distances 1-15
        frames = 8
        
        for distance in distances:
            # Create pattern with specific stack distance
            # Access pages 0-(distance-1), then repeat
            addresses = []
            for cycle in range(10):
                for page in range(distance):
                    addresses.append((page << 8) | 0)
            
            print(f"  Testing stack distance {distance:2d}: ", end="")
            
            for algo in ["FIFO", "LRU", "OPT"]:
                faults = self.run_single_test(f"stack_{distance}", addresses, frames, algo)
                results[algo].append(faults)
                print(f"{algo}={faults:2d} ", end="")
            print()
        
        # Find crossover point where LRU becomes better than FIFO
        crossover = None
        for i, dist in enumerate(distances):
            if results["LRU"][i] < results["FIFO"][i]:
                crossover = dist
                break
        
        if crossover:
            print(f"  ðŸŽ¯ LRU becomes better than FIFO at stack distance {crossover}")
        
        return results
    
    def test_locality_phases(self):
        """Test algorithms with distinct locality phases"""
        print("ðŸ”¬ Micro-benchmark: Locality Phase Changes")
        
        # Create pattern with two distinct phases
        addresses = []
        
        # Phase 1: Heavy use of pages 0-3
        for _ in range(100):
            page = 0 + (_ % 4)  # Round-robin through 0,1,2,3
            addresses.append((page << 8) | 0)
        
        # Transition: Single access to pages 10-15 (destroys cache)
        for page in range(10, 16):
            addresses.append((page << 8) | 0)
        
        # Phase 2: Return to pages 0-3
        for _ in range(50):
            page = 0 + (_ % 4)
            addresses.append((page << 8) | 0)
        
        frames = 4
        print(f"  Pattern: 100 accesses to pages 0-3, then pages 10-15, then 50 more to 0-3")
        print(f"  Testing with {frames} frames:")
        
        for algo in ["FIFO", "LRU", "OPT"]:
            faults = self.run_single_test("locality_phase", addresses, frames, algo)
            print(f"    {algo}: {faults} page faults")
        
        return addresses
    
    def test_scan_resistance(self):
        """Test resistance to sequential scans"""
        print("ðŸ”¬ Micro-benchmark: Sequential Scan Resistance")
        
        addresses = []
        frames = 8
        
        # Working set of pages 0-3 (fits in cache)
        for _ in range(20):
            page = _ % 4
            addresses.append((page << 8) | 0)
        
        # Sequential scan through pages 20-35 (larger than cache)
        for page in range(20, 36):
            addresses.append((page << 8) | 0)
        
        # Return to working set
        for _ in range(20):
            page = _ % 4
            addresses.append((page << 8) | 0)
        
        print(f"  Pattern: Working set [0-3], scan [20-35], return to [0-3]")
        print(f"  Testing with {frames} frames:")
        
        for algo in ["FIFO", "LRU", "OPT"]:
            faults = self.run_single_test("scan_resist", addresses, frames, algo)
            print(f"    {algo}: {faults} page faults")
    
    def test_belady_anomaly_detailed(self):
        """Detailed Belady's anomaly test"""
        print("ðŸ”¬ Micro-benchmark: Belady's Anomaly (Detailed)")
        
        # Classic anomaly sequence
        page_sequence = [1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5]
        addresses = [(page << 8) | 0 for page in page_sequence]
        
        print(f"  Page sequence: {page_sequence}")
        print("  FIFO results by frame count:")
        
        anomaly_detected = False
        prev_faults = 0
        
        for frames in range(3, 8):
            faults = self.run_single_test("belady", addresses, frames, "FIFO")
            direction = ""
            if frames > 3:
                if faults > prev_faults:
                    direction = "ðŸ“ˆ (ANOMALY!)"
                    anomaly_detected = True
                elif faults < prev_faults:
                    direction = "ðŸ“‰"
                else:
                    direction = "âž¡ï¸"
            
            print(f"    {frames} frames: {faults} page faults {direction}")
            prev_faults = faults
        
        if anomaly_detected:
            print("  âœ… Belady's anomaly confirmed!")
        else:
            print("  âŒ No anomaly detected in this range")
    
    def test_optimal_lookahead(self):
        """Test OPT's lookahead advantage"""
        print("ðŸ”¬ Micro-benchmark: OPT Lookahead Advantage")
        
        # Craft a sequence where OPT's lookahead provides clear benefit
        addresses = []
        
        # Fill cache with pages 0-3
        for page in range(4):
            addresses.append((page << 8) | 0)
        
        # Force eviction with page 4
        addresses.append((4 << 8) | 0)
        
        # Now access page 5, forcing another eviction
        # OPT should evict the page that will be used farthest in future
        addresses.append((5 << 8) | 0)
        
        # Future accesses: page 0 will be used soon, page 1 much later
        addresses.append((0 << 8) | 0)  # Page 0 used soon
        addresses.append((2 << 8) | 0)  # Page 2 used soon
        addresses.append((3 << 8) | 0)  # Page 3 used soon
        
        # Much later, access page 1
        for _ in range(10):
            addresses.append((6 << 8) | 0)  # Some other accesses
        addresses.append((1 << 8) | 0)  # Page 1 used much later
        
        frames = 4
        print(f"  Crafted sequence to test lookahead with {frames} frames:")
        
        for algo in ["FIFO", "LRU", "OPT"]:
            faults = self.run_single_test("lookahead", addresses, frames, algo)
            print(f"    {algo}: {faults} page faults")
        
        print("  OPT should perform best by keeping pages needed soonest")
    
    def create_performance_graph(self, results: Dict, test_name: str):
        """Create a performance comparison graph"""
        try:
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(10, 6))
            
            if test_name == "stack_distance":
                distances = range(1, 16)
                for algo in ["FIFO", "LRU", "OPT"]:
                    plt.plot(distances, results[algo], marker='o', label=algo)
                
                plt.xlabel("Stack Distance")
                plt.ylabel("Page Faults")
                plt.title("Page Faults vs Stack Distance")
                plt.legend()
                plt.grid(True)
                plt.savefig("stack_distance_analysis.png")
                print(f"  ðŸ“Š Graph saved as stack_distance_analysis.png")
            
        except ImportError:
            print("  ðŸ“Š matplotlib not available for graphing")

def main():
    if not os.path.exists("memSim") and not os.path.exists("memSim.py"):
        print("âŒ memSim not found. Please build it first with 'make'")
        return
    
    # Create backing store
    with open("BACKING_STORE.bin", 'wb') as f:
        for page in range(256):
            page_data = bytearray([page] * 256)
            f.write(page_data)
    
    bench = MicroBenchmark()
    
    print("=== Virtual Memory Algorithm Micro-Benchmarks ===\n")
    
    # Run micro-benchmarks
    stack_results = bench.test_stack_distance()
    print()
    
    bench.test_locality_phases()
    print()
    
    bench.test_scan_resistance()
    print()
    
    bench.test_belady_anomaly_detailed()
    print()
    
    bench.test_optimal_lookahead()
    print()
    
    # Create performance graph
    bench.create_performance_graph(stack_results, "stack_distance")
    
    print("=== Micro-Benchmark Summary ===")
    print("âœ… Stack distance test shows LRU advantage with temporal locality")
    print("âœ… Phase change test shows algorithm adaptation to workload shifts")
    print("âœ… Scan resistance test demonstrates cache pollution effects")
    print("âœ… Belady's anomaly test confirms FIFO's counter-intuitive behavior")
    print("âœ… Lookahead test demonstrates OPT's theoretical superiority")

if __name__ == "__main__":
    main()